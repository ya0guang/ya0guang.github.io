<!doctype html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="generator" content="TiddlyWiki" />
<meta name="tiddlywiki-version" content="5.3.3" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="apple-mobile-web-app-capable" content="yes" />
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
<meta name="mobile-web-app-capable" content="yes"/>
<meta name="format-detection" content="telephone=no">
<link id="faviconLink" rel="shortcut icon" href="favicon.ico">
<link rel="stylesheet" href="static.css">
<title>Large Language Models for Code: Security Hardening and Adversarial Testing: ya0guang's notebook — Personality Backup</title>
</head>
<body class="tc-body">

<section class="tc-story-river tc-static-story-river">
<p><div class="tc-tiddler-frame tc-tiddler-view-frame tc-tiddler-exists  tc-tagged-LLM tc-tagged-Paper" data-tags="LLM Paper" data-tiddler-title="Large Language Models for Code: Security Hardening and Adversarial Testing" role="article"><div class="tc-tiddler-title"><div class="tc-titlebar"><span class="tc-tiddler-controls"><button aria-expanded="false" aria-label="more" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fmore-tiddler-actions" title="More actions"></button><div class=" tc-reveal" hidden="true"></div><button aria-label="edit" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fedit" title="Edit this tiddler"></button><button aria-label="close" class="tc-btn-invisible tc-btn-%24%3A%2Fcore%2Fui%2FButtons%2Fclose" title="Close this tiddler"></button></span><span><h2 class="tc-title">Large Language Models for Code: Security Hardening and Adversarial Testing</h2></span></div><div class="tc-tiddler-info tc-popup-handle tc-reveal" hidden="true"></div></div><div class="tc-reveal"></div><div class=" tc-reveal"><div class="tc-subtitle"><a class="tc-tiddlylink tc-tiddlylink-missing" href=".html"></a> December 3, 2023 at 10:10 am</div></div><div class=" tc-reveal"><div class="tc-tags-wrapper"><span class="tc-tag-list-item" data-tag-title="LLM"><span aria-expanded="false" class="tc-tag-label tc-btn-invisible" draggable="true" style="fill:rgb(38, 38, 38);color:rgb(38, 38, 38);">LLM</span><span class="tc-drop-down tc-reveal" hidden="true"></span></span><span class="tc-tag-list-item" data-tag-title="Paper"><span aria-expanded="false" class="tc-tag-label tc-btn-invisible" draggable="true" style="fill:rgb(38, 38, 38);color:rgb(38, 38, 38);">Paper</span><span class="tc-drop-down tc-reveal" hidden="true"></span></span></div></div><div class="tc-tiddler-body tc-reveal"><p><a class="tc-tiddlylink-external" href="https://arxiv.org/pdf/2302.05319.pdf" rel="noopener noreferrer" target="_blank">PDF</a></p><p>Propose a system: SVEN: property-specific continuous vectors to guide program generation towards the given property, without modifying the LM’s weights. </p><ul><li>Two studies: security hardening and adversarial testing</li></ul><h2 class="">Goods</h2><ul><li>Great evaluation result</li><li>Lots of efforts</li></ul><h2 class="">Questions</h2><ul><li>Does other methods degrade the performance (i.e., functional correctness?)</li><li>How do they evaluate functional correctness?</li><li>Does the training set contains source code of different lanauges?</li><li>Can we get the same performance via prompt engineering? I.e., using prompts to lead LLMs to the hidden state led by the vectors.</li></ul><h2 class="">Interesting Things</h2><ul><li>The refined model performs better on CWEs which are not covered in the training data.</li></ul><h2 class="">Lacks</h2><ul><li>Understanding: why the model performs well on other CWEs?</li><li>Only a baseline prompt is evaluated. What about more complex prompts?</li></ul></div>
</div></p>
</section>
</body>
</html>

