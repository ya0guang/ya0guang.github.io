created: 20231024195704827
modified: 20231024200300241
tags: Paper LLM Analysis
title: RTFM! Automatic Assumption Discovery and Verification Derivation from Library Document for API Misuse Detection
type: text/vnd.tiddlywiki

* [[PDF|https://www.xiaojingliao.com/uploads/9/7/0/2/97024238/ccs_2020_advance.pdf]]
* [[Source code|https://github.com/lvtao-sec/Advance]]

! Abstract

To use library APIs, a developer is supposed to follow guidance and respect some constraints, which we call integration assumptions (IAs). Violations of these assumptions can have serious consequences, introducing security-critical flaws such as use-after-free, NULL-dereference, and authentication errors. Analyzing a program for compliance with IAs involves significant effort and needs to be automated. A promising direction is to automatically recover IAs from a library document using Natural Language Processing (NLP) and then verify their consistency with the ways APIs are used in a program through code analysis. However, a practical solution along this line needs to overcome several key challenges, particularly the discovery of IAs from loosely formatted documents and interpretation of their informal descriptions to identify complicated constraints (e.g., data-/control-flow relations between different APIs).

In this paper, we present a new technique for automated assumption discovery and verification derivation from library documents. Our approach, called Advance, utilizes a suite of innovations to address those challenges. More specifically, we leverage the observation that IAs tend to express a strong sentiment in emphasizing the importance of a constraint, particularly those security-critical, and utilize a new sentiment analysis model to accurately recover them from loosely formatted documents. These IAs are further processed to identify hidden references to APIs and parameters, through an embedding model, to identify the information-flow relations expected to be followed. Then our approach runs frequent subtree mining to discover the grammatical units in IA sentences that tend to indicate some categories of constraints that could have security implications. These components are mapped to verification code snippets organized in line with the IA sentence's grammatical structure, and can be assembled into verification code executed through CodeQL to discover misuses inside a program. We implemented this design and evaluated it on 5 popular libraries (OpenSSL, SQLite, libpcap, libdbus and libxml2) and 39 real-world applications. Our analysis discovered 193 API misuses, including 139 flaws never reported before.

! System

[img[Screenshot 2023-10-24 at 15.58.16.png]]

# Extract IA from the documents
# Identify the functions referred in the IA
# Generate CodeQL queries of the target API (mis)uses

! Strengthes and Weaknesses

* High accuracy and low FP (how?)
* Identified a lot of previously-unknown bugs
* New dataset
* Relies heavily on well-formed documentation?
* The FN is not very low
* Generating VC takes a long while